# Bulk DOCX to PDF Conversion Service

A production-ready, scalable microservice for converting DOCX files to PDF using FastAPI, Celery, Redis, and LibreOffice. Supports bulk uploads with asynchronous processing and per-file error handling.

---

## ğŸ—ï¸ Architecture Overview

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI API â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    Redis    â”‚
â”‚             â”‚         â”‚   (Port 8000)â”‚         â”‚   (Queue)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                        â”‚
                               â”‚                        â–¼
                               â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚                 â”‚   Celery    â”‚
                               â”‚                 â”‚   Worker    â”‚
                               â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                        â”‚
                               â–¼                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚       PostgreSQL Database        â”‚
                        â”‚    (Job & File Status Storage)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                        â”‚
                               â–¼                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚      Shared Docker Volume        â”‚
                        â”‚   /app/storage (File Storage)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Architecture Decisions

#### 1. **Asynchronous Processing with Celery**
**Why?** The assignment explicitly requires non-blocking job submission.

- **Problem:** Converting large DOCX files can take 10-60 seconds each
- **Solution:** Celery workers process conversions in the background
- **Benefit:** API responds immediately (202 Accepted), users can poll for status

**Flow:**
```
1. Client uploads ZIP â†’ API extracts files â†’ Creates job in DB
2. API enqueues tasks to Redis â†’ Returns job_id immediately
3. Celery workers pick up tasks â†’ Convert DOCX to PDF using LibreOffice
4. Workers update DB status â†’ Trigger finalization when all files done
5. Finalization task creates ZIP of PDFs â†’ Job marked as completed
6. Client downloads ZIP via download endpoint
```

#### 2. **Message Queue (Redis)**
**Why?** Decouples API from workers for scalability.

- **API Service:** Stateless, can scale horizontally
- **Worker Service:** Stateless, can scale horizontally
- **Redis:** Acts as task broker and result backend
- **Benefit:** Can run multiple API instances and multiple workers independently

#### 3. **Shared File Storage (Docker Volume)**
**Why?** Multiple containers need access to the same files.

**Challenge:** API uploads files, workers read them, users download results.

**Solution:** Docker named volume `shared_storage` mounted at `/app/storage` in both API and worker containers.

```yaml
# docker-compose.yml
volumes:
  shared_storage:  # Named volume shared across containers

services:
  api:
    volumes:
      - shared_storage:/app/storage  # Mounted in API
  
  worker:
    volumes:
      - shared_storage:/app/storage  # Mounted in worker
```

**File Structure:**
```
/app/storage/
  â”œâ”€â”€ 1/                    # Job ID 1
  â”‚   â”œâ”€â”€ upload.zip        # Original upload
  â”‚   â”œâ”€â”€ file1.docx        # Extracted DOCX
  â”‚   â”œâ”€â”€ file1.pdf         # Converted PDF
  â”‚   â”œâ”€â”€ file2.docx
  â”‚   â”œâ”€â”€ file2.pdf
  â”‚   â””â”€â”€ converted_files.zip  # Final downloadable ZIP
  â”œâ”€â”€ 2/                    # Job ID 2
  â”‚   â””â”€â”€ ...
```

#### 4. **Database for State Management (PostgreSQL)**
**Why?** Persistent, ACID-compliant storage for job and file status.

- **Jobs Table:** Stores job-level status (pending, processing, completed, failed, partial_success)
- **JobFiles Table:** Stores per-file status and error messages
- **Relationship:** One-to-Many (Job â†’ JobFiles) with cascade delete
- **Benefit:** Allows precise status tracking and error reporting per file

#### 5. **LibreOffice for Conversion**
**Why?** Free, open-source, Linux-compatible DOCX to PDF converter.

**Alternatives considered:**
- `docx2pdf` library: âŒ Requires Microsoft Word (Windows/Mac only)
- `unoconv`: âŒ Deprecated, unreliable
- **LibreOffice (headless):** âœ… Works on Linux, reliable, widely used

**Implementation:**
```python
subprocess.run([
    'libreoffice',
    '--headless',           # No GUI
    '--convert-to', 'pdf',  # Output format
    '--outdir', output_dir, # Where to save PDF
    input_file              # DOCX file path
])
```

#### 6. **Per-File Error Handling**
**Why?** Assignment requires: "A single file failure should not stop the entire job."

**Implementation:**
- Each file conversion is a separate Celery task
- Try-catch around each conversion
- Failed files marked as `FAILED` with error message
- Successful files marked as `COMPLETED`
- Job status determined after all files processed:
  - All success â†’ `COMPLETED`
  - All failed â†’ `FAILED`
  - Mixed â†’ `PARTIAL_SUCCESS`

**Benefit:** Users can download successfully converted files even if some fail.

---

## ğŸ”„ Asynchronous Processing Flow

### Detailed Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CLIENT UPLOADS ZIP                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. API HANDLER (POST /api/v1/jobs)                              â”‚
â”‚    - Validates ZIP file                                         â”‚
â”‚    - Creates Job record (status=PENDING)                        â”‚
â”‚    - Saves ZIP to /app/storage/{job_id}/upload.zip             â”‚
â”‚    - Extracts DOCX files                                        â”‚
â”‚    - Creates JobFile records for each DOCX                      â”‚
â”‚    - Enqueues conversion tasks to Redis                         â”‚
â”‚    - Returns 202 Accepted with job_id                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. REDIS QUEUE                                                  â”‚
â”‚    - Stores tasks: [convert_file_task(file_id=1),              â”‚
â”‚                     convert_file_task(file_id=2), ...]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. CELERY WORKER (Background)                                   â”‚
â”‚    For each file:                                               â”‚
â”‚    - Picks up task from Redis                                   â”‚
â”‚    - Updates JobFile status to PROCESSING                       â”‚
â”‚    - Reads DOCX from /app/storage/{job_id}/{filename}          â”‚
â”‚    - Runs LibreOffice conversion                                â”‚
â”‚    - Saves PDF to /app/storage/{job_id}/{filename}.pdf         â”‚
â”‚    - Updates JobFile status to COMPLETED or FAILED              â”‚
â”‚    - Triggers finalize_job_task                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. FINALIZATION TASK (After each file)                          â”‚
â”‚    - Checks if all files are processed (no PENDING/PROCESSING)  â”‚
â”‚    - If yes:                                                    â”‚
â”‚      * Creates ZIP of all successful PDFs                       â”‚
â”‚      * Determines overall job status                            â”‚
â”‚      * Updates Job status to COMPLETED/FAILED/PARTIAL_SUCCESS   â”‚
â”‚    - If no: Exits (will be triggered again by next file)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. CLIENT POLLS STATUS (GET /api/v1/jobs/{job_id})             â”‚
â”‚    - Returns current job status and per-file breakdown          â”‚
â”‚    - Includes download_url when status is COMPLETED             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. CLIENT DOWNLOADS (GET /api/v1/jobs/{job_id}/download)       â”‚
â”‚    - Streams /app/storage/{job_id}/converted_files.zip         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Design?

1. **Non-blocking:** API returns immediately, doesn't wait for conversion
2. **Scalable:** Can add more workers to process files in parallel
3. **Resilient:** Individual file failures don't crash the entire job
4. **Transparent:** Users can track progress via status endpoint
5. **Efficient:** Workers process files concurrently (Celery handles this)

---

## ğŸ—„ï¸ Database Schema

```sql
-- Jobs Table
CREATE TABLE jobs (
    id SERIAL PRIMARY KEY,
    status VARCHAR(20) NOT NULL,  -- pending, processing, completed, failed, partial_success
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Job Files Table
CREATE TABLE job_files (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    filename VARCHAR(255) NOT NULL,
    status VARCHAR(20) NOT NULL,  -- pending, processing, completed, failed
    error_message TEXT NULL
);
```

**Design Notes:**
- `ON DELETE CASCADE`: Deleting a job deletes all its files
- `created_at`: Useful for cleanup jobs (delete old jobs after X days)
- `error_message`: Stores conversion errors for debugging

---

## ğŸ“¡ API Endpoints

### Base URL
```
http://localhost:8000
```

### 1. Health Check
```http
GET /
```

**Response (200 OK):**
```json
{
  "message": "Service is running successfully"
}
```

---

### 2. Submit Conversion Job
```http
POST /api/v1/jobs/
Content-Type: multipart/form-data
```

**Request:**
- **Body:** `file` (ZIP file containing DOCX files)

**Example (cURL):**
```bash
curl -X POST "http://localhost:8000/api/v1/jobs/" \
  -F "file=@documents.zip"
```

**Response (202 Accepted):**
```json
{
  "job_id": 1,
  "status": "pending",
  "message": "Job created successfully"
}
```

**Error Responses:**
- `400 Bad Request`: Invalid file type, empty ZIP, no DOCX files
- `500 Internal Server Error`: Server error during processing

---

### 3. Get Job Status
```http
GET /api/v1/jobs/{job_id}
```

**Example:**
```bash
curl http://localhost:8000/api/v1/jobs/1
```

**Response (200 OK):**

**Pending/Processing:**
```json
{
  "id": 1,
  "status": "pending",
  "created_at": "2025-12-15T14:37:16.609641Z",
  "files": [
    {
      "id": 1,
      "filename": "report.docx",
      "status": "processing",
      "error_message": null
    },
    {
      "id": 2,
      "filename": "proposal.docx",
      "status": "pending",
      "error_message": null
    }
  ],
  "download_url": null
}
```

**Completed:**
```json
{
  "id": 1,
  "status": "completed",
  "created_at": "2025-12-15T14:37:16.609641Z",
  "files": [
    {
      "id": 1,
      "filename": "report.docx",
      "status": "completed",
      "error_message": null
    },
    {
      "id": 2,
      "filename": "proposal.docx",
      "status": "completed",
      "error_message": null
    }
  ],
  "download_url": "/api/v1/jobs/1/download"
}
```

**Partial Success (some files failed):**
```json
{
  "id": 1,
  "status": "partial_success",
  "created_at": "2025-12-15T14:37:16.609641Z",
  "files": [
    {
      "id": 1,
      "filename": "report.docx",
      "status": "completed",
      "error_message": null
    },
    {
      "id": 2,
      "filename": "corrupted.docx",
      "status": "failed",
      "error_message": "LibreOffice conversion failed: Invalid file format"
    }
  ],
  "download_url": "/api/v1/jobs/1/download"
}
```

**Error Responses:**
- `404 Not Found`: Job ID doesn't exist
- `422 Unprocessable Entity`: Invalid job ID format

---

### 4. Download Converted Files
```http
GET /api/v1/jobs/{job_id}/download
```

**Example:**
```bash
curl -O http://localhost:8000/api/v1/jobs/1/download
```

**Response (200 OK):**
- **Content-Type:** `application/zip`
- **Body:** ZIP file containing all successfully converted PDFs

**Error Responses:**
- `400 Bad Request`: Job not ready for download (still processing)
- `404 Not Found`: Job doesn't exist or result file not found

---

### 5. API Documentation (Swagger UI)
```http
GET /docs
```

Interactive API documentation with "Try it out" functionality.

---

## ğŸš€ Running the Project

### Prerequisites
- Docker Desktop installed and running
- 8GB+ RAM recommended
- Ports 8000, 5432, 6379 available

### Quick Start

1. **Clone the repository:**
```bash
cd /path/to/Backend-Repo
```

2. **Start all services:**
```bash
docker-compose up --build
```

This starts:
- **PostgreSQL** (port 5432): Database
- **Redis** (port 6379): Message queue
- **API** (port 8000): FastAPI application
- **Worker**: Celery worker for background processing

3. **Wait for services to be ready:**
```
âœ” Container backend-repo-db-1      Healthy
âœ” Container backend-repo-redis-1   Started
âœ” Container backend-repo-api-1     Started
âœ” Container backend-repo-worker-1  Started
```

4. **Verify API is running:**
```bash
curl http://localhost:8000/
# Response: {"message": "Service is running successfully"}
```

5. **Access API documentation:**
```
http://localhost:8000/docs
```

### Testing the Service

#### Quick Manual Test
```bash
./quick-test.sh
```

This script:
- Creates sample DOCX files
- Submits a conversion job
- Polls for completion
- Downloads and verifies results

#### Full Test Suite (31 tests)
```bash
./run-tests.sh
```

Or manually:
```bash
pip install -r requirements-test.txt
pytest tests/ -v
```

### Stopping the Service

```bash
# Stop containers
docker-compose down

# Stop and remove all data (volumes)
docker-compose down -v
```

---

## ğŸ› ï¸ Local Development (Without Docker)

### Prerequisites
- Python 3.10+
- PostgreSQL installed and running
- Redis installed and running
- LibreOffice installed

### Setup

1. **Install dependencies:**
```bash
pip install -r requirements.txt
```

2. **Set environment variables:**
```bash
export DATABASE_URL="postgresql://user:password@localhost:5432/docx_converter"
export REDIS_URL="redis://localhost:6379/0"
export FILE_STORAGE_PATH="storage"
```

3. **Create database:**
```bash
createdb docx_converter
```

4. **Run API server:**
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

5. **Run Celery worker (in separate terminal):**
```bash
celery -A app.celery_app worker --loglevel=info
```

---

## ğŸ“ Project Structure

```
Backend-Repo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ celery_app.py        # Celery configuration
â”‚   â”œâ”€â”€ config.py             # Environment settings
â”‚   â”œâ”€â”€ database.py           # SQLAlchemy setup
â”‚   â”œâ”€â”€ models.py             # Database models (Job, JobFile)
â”‚   â”œâ”€â”€ schemas.py            # Pydantic schemas for API
â”‚   â”œâ”€â”€ tasks.py              # Celery task: convert_file_task
â”‚   â”œâ”€â”€ finalize_task.py      # Celery task: finalize_job_task
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ jobs.py           # API endpoints
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py           # API endpoint tests (25 tests)
â”‚   â””â”€â”€ test_models.py        # Database model tests (6 tests)
â”œâ”€â”€ main.py                   # FastAPI application entry point
â”œâ”€â”€ docker-compose.yml        # Docker orchestration
â”œâ”€â”€ Dockerfile                # Container definition
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ requirements-test.txt     # Test dependencies
â”œâ”€â”€ pytest.ini                # Pytest configuration
â”œâ”€â”€ quick-test.sh             # Quick manual test script
â”œâ”€â”€ run-tests.sh              # Test suite runner
â””â”€â”€ README.md                 # This file
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `DATABASE_URL` | `sqlite:///./test.db` | PostgreSQL connection string |
| `REDIS_URL` | `redis://localhost:6379/0` | Redis connection string |
| `FILE_STORAGE_PATH` | `storage` | Directory for file storage |
| `APP_NAME` | `Bulk DOCX to PDF Service` | Application name |
| `DEBUG` | `False` | Debug mode |

### Docker Compose Configuration

```yaml
# Key configurations in docker-compose.yml

# Shared storage volume
volumes:
  shared_storage:  # Accessible by both API and worker

# API service
api:
  environment:
    - DATABASE_URL=postgresql://user:password@db:5432/docx_converter
    - REDIS_URL=redis://redis:6379/0
    - FILE_STORAGE_PATH=/app/storage
  volumes:
    - shared_storage:/app/storage  # Mounted volume

# Worker service
worker:
  environment:
    - DATABASE_URL=postgresql://user:password@db:5432/docx_converter
    - REDIS_URL=redis://redis:6379/0
    - FILE_STORAGE_PATH=/app/storage
  volumes:
    - shared_storage:/app/storage  # Same volume as API
```

---

## ğŸ¯ Design Highlights

### 1. Scalability
- **Stateless API:** Can run multiple instances behind a load balancer
- **Stateless Workers:** Can scale workers independently based on load
- **Shared Storage:** Docker volume accessible by all containers
- **Message Queue:** Decouples API from workers

### 2. Reliability
- **Per-file error handling:** One bad file doesn't fail the entire job
- **Database persistence:** Job status survives container restarts
- **Timeout handling:** 60-second timeout per file conversion
- **Graceful degradation:** Partial success allows downloading successful files

### 3. Observability
- **Detailed status tracking:** Job-level and file-level status
- **Error messages:** Specific error details for failed files
- **Timestamps:** Created_at for all jobs
- **Logging:** Structured logging in workers

### 4. User Experience
- **Immediate response:** 202 Accepted, no waiting
- **Progress tracking:** Poll status endpoint for updates
- **Partial results:** Download successful files even if some fail
- **Clear errors:** Specific error messages per file

---

## ğŸ§ª Testing

### Test Coverage
- **Total Tests:** 31
- **API Tests:** 25 (all endpoints, error cases, edge cases)
- **Model Tests:** 6 (database models, relationships)
- **Coverage:** ~90% of application code

### Running Tests
```bash
# Quick test
./quick-test.sh

# Full suite
./run-tests.sh

# Specific test file
pytest tests/test_api.py -v

# With coverage
pytest tests/ --cov=app --cov-report=html
```

See [TEST_DOCUMENTATION.md](TEST_DOCUMENTATION.md) for detailed test information.

---

## ğŸš€ Production Deployment

### Deployment Checklist
- [ ] Set strong database passwords
- [ ] Use managed PostgreSQL (AWS RDS, etc.)
- [ ] Use managed Redis (AWS ElastiCache, etc.)
- [ ] Set up persistent volume for file storage
- [ ] Configure environment variables
- [ ] Set up monitoring (Prometheus, Grafana)
- [ ] Configure log aggregation (ELK, CloudWatch)
- [ ] Set up health checks
- [ ] Configure auto-scaling for workers
- [ ] Set up backup strategy for database

### Recommended Platforms
- **Railway:** Easiest, one-click deploy
- **Render:** Good for microservices
- **AWS ECS/Fargate:** Full control, scalable
- **Google Cloud Run:** Serverless option

---

## ğŸ“ License

This project is created as an assignment submission.

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ for the Backend Developer Technical Assignment
# DocxConversion_Assignment
